{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing libraries"},{"metadata":{"trusted":false},"cell_type":"code","source":"import glob","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import nltk,re,string\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom heapq import nlargest\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lemmatizing the text"},{"metadata":{"trusted":false},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer \n\nclass LemmaTokenizer(object):\n    def __init__(self):\n        self.lemmatizer = WordNetLemmatizer()\n    def __call__(self, articles):\n        return [self.lemmatizer.lemmatize(t) for t in word_tokenize(articles)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Merges an acronyms in a given sentence"},{"metadata":{"trusted":false},"cell_type":"code","source":"def merge_acronyms(s):\n    \"\"\"Merges all acronyms in a given sentence. For example M.I.T -> MIT\"\"\"\n    \n    s = re.sub(r'(?<!\\w)([A-Z])\\.', r'\\1', s)\n    return s","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing to remove unwanted punctuations"},{"metadata":{"trusted":false},"cell_type":"code","source":"def preprocessing(line):\n    line = line.lower()\n    braces = ['[',']','{','}','(',')','-','...','_']\n    line = re.sub(r\"[0-9]\".format(string.punctuation), \" \", line)\n    line = merge_acronyms(line)\n    for b in braces:\n        line = line.replace(b,'')\n    return line","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ranking the sentences according to the Tf-idf scores"},{"metadata":{"trusted":false},"cell_type":"code","source":"def maxTfIdfIndex(sentence_list, feature_names, tfidf):\n    \n    word_frequencies = {}\n    doc = 0\n    lemma = LemmaTokenizer()\n    feature_index = tfidf[doc,:].nonzero()[1]\n    tfidf_scores = zip(feature_index, [tfidf[doc, x] for x in feature_index])\n\n    for w, s in [(feature_names[j], s) for (j, s) in tfidf_scores]:\n        word_frequencies[w] = s\n\n    sum_all_words = sum(word_frequencies.values())\n    #print(sum_all_words)\n    sen1 = \"\"\n    \n    sentence_scores = {}  \n    for sent in sentence_list: \n        sent = preprocessing(sent)\n        #nltk.word_tokenize(sent.lower())\n        sen1 = sent[:]\n        for word in lemma.__call__(sen1):\n            if word in word_frequencies.keys():\n                if sent not in sentence_scores.keys():\n                    # sentence_score = [sum(tf-idf of all words of sentence) / sum(tf-idf of all words of document)] \n                    sentence_scores[sent] = word_frequencies[word]\n                else:\n                    sentence_scores[sent] += word_frequencies[word]\n\n        if sent in sentence_scores.keys(): \n            sentence_scores[sent] = sentence_scores[sent]/sum_all_words\n        \n    key_list = list(sentence_scores.keys()) \n    val_list = list(sentence_scores.values()) \n  \n    #print(key_list[val_list.index(max(val_list))]) \n    #print(val_list.index(max(val_list)))\n    #print()\n    return val_list.index(max(val_list))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Determining the optimal number of clusters using combination of Silhouette Analysis and Thumb Rule"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import silhouette_score\nimport math\n\ndef cluster_no(X, text_size):\n    \"\"\"if (text_size <100):\n        no_of_clusters = text_size\n    else:\n        no_of_clusters = int(math.sqrt(text_size//2))+10\n    \"\"\"\n    no_of_clusters = text_size\n    score_list = [0 for i in range(no_of_clusters)]\n    for i in range(2,no_of_clusters):\n        print(i,end=\" \")\n        kmeans = KMeans(n_clusters = i, init='k-means++', max_iter=100, n_init=1).fit(X)\n        y_pred = list(kmeans.predict(tfidf))\n        score = silhouette_score(X, y_pred, metric='euclidean')\n        score_list[i] = score\n        \n        #print(\"score for the n_cluster {} is {}\".format(i,score))\n    # Visualizing the Silhouette score.\n    plt.scatter(list(range(no_of_clusters)), score_list, c = \"r\")\n    plt.plot(list(range(no_of_clusters)), score_list, c = \"r\")\n    plt.xlabel(\"Number of clusters\")\n    plt.ylabel(\"Silhouette Score\")\n    plt.title(\"Silhouette Score vs no. of clusters\")\n    plt.show()\n\n    return (score_list.index(max(score_list)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading Multiple documents for Current working directory"},{"metadata":{"trusted":false},"cell_type":"code","source":"all_doc_data = []\npath = input(\"Enter path:\")\nfor filename in glob.glob(path+'/*.txt'):\n    with open(filename,'r') as file:\n        text = file.read()\n        if len(text) > 0:\n            all_doc_data.append(text)\nprint(len(all_doc_data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Performing actual Summarization of individual document according to their TF-IDF scores"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.datasets import fetch_20newsgroups\n\nclusters_list = []\ncount_lines_doc = 0\nsent_lines = []\nfinal_sum = []\nfor i in range(len(all_doc_data)):\n    doc = i\n    print(\"Document  {}\".format(i+1))\n    print()\n    #newsgroups_train = fetch_20newsgroups(subset='train', categories=['alt.atheism', 'sci.space'])\n    sentences_list = []\n    sentenc_list = nltk.sent_tokenize(all_doc_data[i])\n    count_lines_doc += len(sentenc_list)\n    print(\"Number of Lines in Actual text\")\n    print(len(sentenc_list))\n    \n    for i in range(len(sentenc_list)):\n        if sentenc_list[i] not in sent_lines:\n            sent_lines.append(sentenc_list[i])\n            sentences_list.append(sentenc_list[i])\n            \n    #sentences_list = newsgroups_train.data\n    if(len(sentences_list)>2):\n        vectorizer = TfidfVectorizer(analyzer='word',min_df = 0,preprocessor=preprocessing,stop_words='english',tokenizer=LemmaTokenizer(), sublinear_tf=True)\n        vectorizer = vectorizer.fit(sentences_list)\n        tfidf = vectorizer.transform(sentences_list)\n        feature_names = vectorizer.get_feature_names()\n        #print(feature_names)\n\n        #Clustering the sentences using KMeans clustering\n        n_cluster = cluster_no(tfidf, len(sentences_list))\n        clusters_list.append(n_cluster)\n        print(\"Optimal cluster number is \",n_cluster)\n        print()\n        kmeans = KMeans(n_clusters=n_cluster, init='k-means++', max_iter=100, n_init=1).fit(tfidf)\n        kmean_indices = list(kmeans.fit_predict(tfidf))\n        #Extracting sentences  with highest tf-idf values\n    \n        ind = maxTfIdfIndex(sentences_list, feature_names, tfidf)\n        cluster_max = [] \n\n        index = kmean_indices[ind]\n        sent = []\n\n        for i in range(len(sentences_list)):\n            if kmean_indices[i] == index:\n                sent.append(sentences_list[i])\n                #print(sentences_list[i])\n        print(\"Number of Lines in Summarized text \")\n        print(len(sent))\n        print()\n        \n        final_sum.extend(sent)\n        summary = ' '.join(sent)\n        \n        #file1 = open(path+'/SummarizedText/summarized_text'+str(i+1)+'.txt','w+')\n        #file1.write(summary)\n        #file1.close()\n        print(summary)\n        print()\n    else:\n        final_sum.extend(sentences_list)\n        print(\"Number of Lines in Summarized text \")\n        print(len(sentences_list))\n        print(' '.join(sentences_list))\n        clusters_list.append(None)\n        \nprint(\"No. of lines in actual document is \",count_lines_doc)\nprint(\"No of lines in summarized document is \",len(final_sum))\nprint(\"Final summary of the Multi-Document is \")\nprint(' '.join(final_sum))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing the data"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\nfor i in range(len(all_doc_data)):\n    print(\"Document  {}\".format(i+1))\n    print()\n    if clusters_list[i] != None :\n        sentences_list = nltk.sent_tokenize(all_doc_data[i])\n        #sentences_list = newsgroups_train.data\n        \n        # vectorizing the Document\n    \n        vectorizer = TfidfVectorizer(analyzer='word',min_df = 0,preprocessor=preprocessing,stop_words='english',tokenizer=LemmaTokenizer(), sublinear_tf=True)\n        tfidf = vectorizer.fit_transform(sentences_list)\n    \n        #Clustering the sentences using KMeans clustering\n        kmeans = KMeans(n_clusters=clusters_list[i], init='k-means++', max_iter=100, n_init=1).fit(tfidf)\n        kmean_indices = kmeans.fit_predict(tfidf)\n    \n        #Decomposing the sparse matrix into list . Inplace of  todense() toarray() can be used\n        pca = PCA(n_components=2)\n        scatter_plot_points = pca.fit_transform(tfidf.todense())\n    \n        #Assigning 1st column to x_axis\n        x_axis = [o[0] for o in scatter_plot_points]\n    \n        #Assigning 2nd column to y_axis\n        y_axis = [o[1] for o in scatter_plot_points]\n\n    \n        colors = [\"r\", \"m\",\"g\",\"c\",\"y\",\"k\",\"w\"]*((clusters_list[i]//7)+1)\n        fig, ax = plt.subplots(figsize=(8,4))\n        ax.scatter(x_axis, y_axis, c=[colors[d] for d in kmean_indices])\n\n        #Plotting centroids in the subplot\n        centers2D = pca.transform(kmeans.cluster_centers_)\n        ax.scatter(centers2D[:,0], centers2D[:,1], marker='x', s=200, linewidths=3, c=\"b\") \n    \n    \n        #for i, txt in enumerate(sentences_list):\n            #ax.annotate(txt, (x_axis[i], y_axis[i]))\n        plt.show()\n    else:\n        print(\"Visualization is not possible for text with redundant data\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Deleting variables after use"},{"metadata":{"trusted":false},"cell_type":"code","source":"del(all_doc_data)\ndel(clusters_list)\ndel(count_lines_doc)\ndel(sent_lines)\ndel(final_sum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"collected = gc.collect()\nprint (\"Garbage collector: collected {} objects.\".format(collected))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}